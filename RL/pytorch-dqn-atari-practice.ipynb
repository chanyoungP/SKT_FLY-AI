{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250b088e",
   "metadata": {},
   "source": [
    "# DQN으로 Atari 게임 강화학습\n",
    "\n",
    "먼저 여러가지 설정 변수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym[atari,accept-rom-license]==0.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be000272-b287-41c7-b5ed-1bd352af1a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 64  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3fe92",
   "metadata": {},
   "source": [
    "### Atari 게임 환경\n",
    "\n",
    "Gym env 의 observation\n",
    "- Observation의 모양: (210, 160, 3)\n",
    "\n",
    "액션 정의\n",
    "- 0: NOOP\n",
    "- 1: FIRE\n",
    "- 2: RIGHT\n",
    "- 3: LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bdb4550-1558-4ba7-a910-9faa34652511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# Use the Baseline Atari environment because of Deepmind helper functions\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477cab4",
   "metadata": {},
   "source": [
    "### 네트워크 정의하기\n",
    "\n",
    "참고: Conv2d 파라미터\n",
    "* in_channels (int) – Number of channels in the input image\n",
    "* out_channels (int) – Number of channels produced by the convolution\n",
    "* kernel_size (int or tuple) – Size of the convolving kernel\n",
    "* stride (int or tuple, optional) – Stride of the convolution. Default: 1\n",
    "* padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n",
    "* padding_mode (str, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85ef96c-cc48-426a-b2f3-12e002502bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "\n",
    "class QModel(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(QModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        action = self.fc2(x)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a23e4c",
   "metadata": {},
   "source": [
    "### 모델 빌딩 & 로스 및 최적화 계산기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = QModel(num_actions)\n",
    "\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = QModel(num_actions)\n",
    "\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859f550",
   "metadata": {},
   "source": [
    "### Replay Buffer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eed8ce4-5a53-457f-bc40-fda577b2ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 100000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 500000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda734a1",
   "metadata": {},
   "source": [
    "### 전처리\n",
    "\n",
    "Observation 을 QModel의 입력 타입으로 전처리\n",
    "\n",
    "- PIL Image 객체로 바꾸고\n",
    "- 그레이 스케일로 바꾸고\n",
    "- 84 * 84 짜리 이미지로 리사이징\n",
    "- 지금까지 np.array이니 torch.tensor로 캐스팅\n",
    "- 마지막으로 batch 축 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf51fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Grayscale(),\n",
    "    T.Resize((84, 84)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Function to preprocess the state\n",
    "def preprocess_state(state):\n",
    "    state = preprocess(state).unsqueeze(0)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61693680",
   "metadata": {},
   "source": [
    "### Epsilon-greedy 액션 선택 함수\n",
    "\n",
    "학습시 에피소드 생성하면서 사용 (주의: 입력은 batch axis 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f9bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select an action\n",
    "def get_greedy_epsilon(model, state):\n",
    "    global epsilon\n",
    "    \n",
    "    #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n",
    "    if np.random.rand(1)[0] < epsilon:\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            # add a batch axis\n",
    "            #state = state.unsqueeze(0)\n",
    "            # compute the q-values\n",
    "            q_values = model(state)\n",
    "            # the action of maximum q-value\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "    # decay epsilon\n",
    "    epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "    epsilon = max(epsilon, epsilon_min)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6728b0",
   "metadata": {},
   "source": [
    "### Greedy 액션 선택 함수\n",
    "\n",
    "나중에 evaluation 시 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8025c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_action(model, state):\n",
    "    global epsilon\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #state = state.unsqueeze(0) # batch dimension\n",
    "        q_values = model(state)\n",
    "        action = q_values.argmax().item()\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ac400",
   "metadata": {},
   "source": [
    "### Update 파트\n",
    "\n",
    "- Replay buffer 에서 batch하나를 샘플링하고,\n",
    "- model을 update한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49c6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a batch of _batch_size from replay buffers\n",
    "# return numpy.ndarrays\n",
    "def sample_batch(_batch_size):\n",
    "    # Get indices of samples for replay buffers\n",
    "    indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n",
    "\n",
    "    state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n",
    "    state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n",
    "    rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n",
    "    action_sample = np.array([action_history[i] for i in indices])\n",
    "    done_sample = np.array([float(done_history[i]) for i in indices])\n",
    "\n",
    "    return state_sample, state_next_sample, rewards_sample, action_sample, done_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97e7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the Q-network\n",
    "def update_network():\n",
    "    # sample a batch of ...\n",
    "    state_sample, state_next_sample, rewards_sample, action_sample, done_sample = \\\n",
    "        sample_batch(batch_size)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    state_sample = torch.tensor(state_sample, dtype=torch.float32)\n",
    "    state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32)\n",
    "    action_sample = torch.tensor(action_sample, dtype=torch.int64)\n",
    "    rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32)\n",
    "    done_sample = torch.tensor(done_sample, dtype=torch.float32)\n",
    "\n",
    "    # Compute the target Q-values for the states\n",
    "    with torch.no_grad():\n",
    "        future_rewards = model_target(state_next_sample)\n",
    "        \n",
    "        # compute the q-value for the next state and the action maximizing the q-value\n",
    "        max_q_values = future_rewards.max(dim=1).values\n",
    "        \n",
    "        # compute the target q-value\n",
    "        # if the step was final, max_q_values should not be added\n",
    "        target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n",
    "\n",
    "    # It's forward propagation! Compute the Q-values for the taken actions\n",
    "    q_values = model(state_sample)\n",
    "    q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_function(q_values_action, target_q_values)\n",
    "\n",
    "    # Perform the optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed4dcc-f24e-4990-879a-c2f6af51a063",
   "metadata": {},
   "source": [
    "# Run DQN Tranining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43c0a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Frame count: 6499, Running reward: 1.0\n",
      "Episode: 20, Frame count: 13280, Running reward: 1.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m# Update every fourth frame and once batch size is over 32\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m frame_count \u001b[39m%\u001b[39m update_after_actions \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(done_history) \u001b[39m>\u001b[39m batch_size:\n\u001b[0;32m---> 30\u001b[0m     update_network()\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m frame_count \u001b[39m%\u001b[39m update_target_network \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m     model_target\u001b[39m.\u001b[39mload_state_dict(model\u001b[39m.\u001b[39mstate_dict())\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mupdate_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Compute the target Q-values for the states\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     future_rewards \u001b[39m=\u001b[39m model_target(state_next_sample)\n\u001b[1;32m     18\u001b[0m     \u001b[39m# compute the q-value for the next state and the action maximizing the q-value\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     max_q_values \u001b[39m=\u001b[39m future_rewards\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mQModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n\u001b[1;32m     17\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[0;32m---> 18\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(x))\n\u001b[1;32m     19\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[1;32m     20\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:  # Run until solved\n",
    "    state, info = env.reset()\n",
    "    state, reward, done, _, info = env.step(1)\n",
    "    state = preprocess_state(state)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        # Select an action\n",
    "        action = get_greedy_epsilon(model, state)\n",
    "\n",
    "        # Take the selected action\n",
    "        state_next, reward, done, _, info = env.step(action)\n",
    "        state_next = preprocess_state(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        rewards_history.append(reward)\n",
    "        done_history.append(done)\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            update_network()\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_count += 1\n",
    "    episode_reward_history.append(episode_reward)\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f\"Episode: {episode_count}, Frame count: {frame_count}, Running reward: {running_reward}\")\n",
    "\n",
    "    if episode_count % 5000 == 0:\n",
    "        torch.save(model, 'model.{}'.format(episode_count))\n",
    "    if running_reward > 20:\n",
    "        print(f\"Solved at episode {episode_count}!\")\n",
    "        break\n",
    "    if episode_count % 200 == 0:\n",
    "        break\n",
    "\n",
    "\n",
    "torch.save(model, 'model.final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984b880-e427-48cb-bf91-13a91d6529f5",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c198b4e-b0e4-4fd4-821d-be602c2dbc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import imageio\n",
    "\n",
    "anim_file = 'atari.gif'\n",
    "\n",
    "turn =  0\n",
    "board, info = env.reset()\n",
    "state = preprocess_state(board)\n",
    "board, reward, done, _, info = env.step(1)\n",
    "state = preprocess_state(board)\n",
    "plt.imshow(board)\n",
    "plt.savefig('image_at_turn_{:04d}.png'.format(turn))\n",
    "\n",
    "for timestep in range(1, 100):\n",
    "    turn += 1\n",
    "    action = get_greedy_action(model, state.to(device))\n",
    "    print(action)\n",
    "    board, reward, done, _, info = env.step(action)\n",
    "    state = preprocess_state(board)\n",
    "    plt.imshow(board)\n",
    "    plt.savefig('image_at_turn_{:04d}.png'.format(turn))\n",
    "\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate animated gif file\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('image_at_turn_*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
